{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12Te1uxq-8h0KGxO00WCmXcQVQgBZ1pfv","timestamp":1691362714888},{"file_id":"1UgfyFDfDt3bzxIT9iLaUmhlStYhG5D7a","timestamp":1690558582109},{"file_id":"1LYC7_TKA6D6oHXC4nTfraMiuAkWWJ24t","timestamp":1690143907290},{"file_id":"17V4YnDOPCyqCv4eBZlgGxV4jxwgzs8tG","timestamp":1689343812092}],"toc_visible":true,"authorship_tag":"ABX9TyO9+PfcMO3wrl8FP/228ulF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Real time video demo for Face Emotion Recognition"],"metadata":{"id":"2HGQCLq86wQg"}},{"cell_type":"markdown","source":["This part set up an environment where you can process and manipulate images, display various types of content within the notebook, and handle HTML and JavaScript interactions."],"metadata":{"id":"gCMmTETQGLVU"}},{"cell_type":"code","source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"58mH_LWxb3_g","executionInfo":{"status":"ok","timestamp":1691871334780,"user_tz":240,"elapsed":596,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["The js_to_image function is used to convert images received from the JavaScript side into a format suitable for processing with OpenCV in Python. Conversely, the bbox_to_bytes function takes processed bounding box information and converts it into a format suitable for overlaying on the video stream in the browser."],"metadata":{"id":"YCzKtBGeLcb_"}},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"1yzpdrF7b4_o","executionInfo":{"status":"ok","timestamp":1691871342210,"user_tz":240,"elapsed":166,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["This part of code essentially sets up the face_cascade object to use the Haar Cascade classifier for detecting frontal faces, and it's initialized with the pre-trained model XML file. This object can then be used to detect faces in images or video streams."],"metadata":{"id":"4ZNyIalQRE3G"}},{"cell_type":"code","source":["# initialize the Haar Cascade face detection model\n","face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"],"metadata":{"id":"XoUkISHMb_G2","executionInfo":{"status":"ok","timestamp":1691871349410,"user_tz":240,"elapsed":130,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["This code is designed to create an interactive live video stream using the webcam as input within a Jupyter notebook environment. It bridges the gap between Python and JavaScript to manage the video stream, capture frames, and provide the captured frame data back to the Python environment for analysis or further processing."],"metadata":{"id":"inwxa79gSFbt"}},{"cell_type":"code","source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","\n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"kZDjyOukcPfm","executionInfo":{"status":"ok","timestamp":1691871354334,"user_tz":240,"elapsed":144,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":[" This part loads the pre-trained neural network model from a specific path in the Drive, defines a list of emotion labels, and assigns the emotion labels to a variable for later use in classification tasks."],"metadata":{"id":"3--eR2BFSTtf"}},{"cell_type":"code","source":["# Ensure the /content/model folder exists\n","import os\n","if not os.path.exists('/content/model'):\n","    os.makedirs('/content/model')\n","\n","\n","!pip install gdown\n","import gdown\n","\n","url = 'https://drive.google.com/uc?id=1PFF6omNRkpwRRKSVQIYRHBR1IA5kyQPU'\n","output = '/content/model/modelv10.h5'\n","gdown.download(url, output, quiet=False)\n","\n","from tensorflow.keras.models import load_model\n","model = load_model('/content/model/modelv10.h5')\n","\n","emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","class_labels = emotions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KW45d4_qEKA2","executionInfo":{"status":"ok","timestamp":1691871375775,"user_tz":240,"elapsed":13406,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}},"outputId":"143071df-7c1c-4bd8-e765-257741d077a8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1PFF6omNRkpwRRKSVQIYRHBR1IA5kyQPU\n","To: /content/model/modelv10.h5\n","100%|██████████| 55.0M/55.0M [00:00<00:00, 112MB/s]\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"w6TiTT0OKU3R"}},{"cell_type":"markdown","source":["This code captures the webcam video stream, detects faces, classifies the emotions associated with the detected faces, overlays bounding boxes and emotion labels on the frames, and tracks the frequency of each detected emotion."],"metadata":{"id":"aqIpelGjTNFR"}},{"cell_type":"code","source":["# Initialize a dictionary to keep track of the frequency of each emotion detected\n","emotion_counts = {emotion: 0 for emotion in emotions}\n","\n","from numpy.core.multiarray import result_type\n","# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","\n","    # Process each detected face\n","    for (x, y, w, h) in faces:\n","        # Crop the detected face from the image\n","        cropped_face = img[y:y+h, x:x+w]\n","\n","        # Resize the cropped face to match the input size of your FER model (e.g., 48x48)\n","        resized_face = cv2.resize(cropped_face, (48, 48))\n","\n","        # Convert the resized face to grayscale\n","        gray_resized = cv2.cvtColor(resized_face, cv2.COLOR_RGB2GRAY)\n","\n","        # Normalize pixel values to range between 0 and 1\n","        normalized = gray_resized / 255.0\n","\n","        # Expand dimensions to match the shape that model expects\n","        normalized = np.expand_dims(normalized, axis=-1)\n","        normalized = np.expand_dims(normalized, axis=0)\n","\n","        # Perform prediction using your FER model\n","        result = model.predict(normalized, verbose=0)\n","        emotion_result = class_labels[np.argmax(result)]\n","\n","        # Update the emotion_counts dictionary\n","        emotion_counts[emotion_result] += 1\n","\n","\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","\n","  #Use puText() method for\n","  #inserting text on video\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","    # get face bounding box for overlay\n","    for (x,y,w,h) in faces:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","      cv2.putText(bbox_array, str(emotion_result), (0,50), font, 1, (0, 0, 255), 2, cv2.LINE_4);\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","\n","# After the loop ends, find the emotion with the highest frequency\n","most_dominant_emotion = max(emotion_counts, key=emotion_counts.get)\n","\n","print(\"The most dominant emotion detected during the camera capture was:\", most_dominant_emotion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"n7-hS9LZcVsg","outputId":"51da9a7b-82a3-42e7-8640-1a94c956d449","executionInfo":{"status":"ok","timestamp":1691871422989,"user_tz":240,"elapsed":39406,"user":{"displayName":"Julio C Murillo","userId":"08014328648915552430"}}},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The most dominant emotion detected during the camera capture was: Neutral\n"]}]}]}